{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "034b6942-19fe-4772-a97c-f330eb1072a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 13:37:15,359 - __main__ - INFO - üèÅ D√©but du pipeline\n",
      "2025-05-17 13:37:16,113 - __main__ - INFO -  ---Chargement de dim_produit dans SQL Server---\n",
      "2025-05-17 13:37:20,445 - __main__ - INFO -  ---Chargement de dim_entrepot dans SQL Server---\n",
      "2025-05-17 13:37:21,068 - __main__ - INFO -  ---Chargement de stg_commandes dans SQL Server---\n",
      "2025-05-17 13:37:21,423 - __main__ - INFO -  ---Chargement de stg_livrasion dans SQL Server---\n",
      "2025-05-17 13:37:21,686 - __main__ - INFO -  ---Chargement de stg_mouvements dans SQL Server---\n",
      "2025-05-17 13:37:21,821 - __main__ - INFO -  ---Chargement de FACT_Mouvement dans SQL Server---\n",
      "2025-05-17 13:37:22,055 - __main__ - INFO -  ---Chargement de FACT_Livraison dans SQL Server---\n",
      "2025-05-17 13:37:22,278 - __main__ - INFO -  ---Chargement de FACT_commandes dans SQL Server---\n",
      "2025-05-17 13:37:22,575 - __main__ - INFO - üì§ Uploading FACT_Mouvement to BigQuery ‚Üí pipeline-458019.vente.FACT_Mouvement...\n",
      "2025-05-17 13:37:27,716 - __main__ - INFO - ‚úÖ Table FACT_Mouvement uploaded successfully.\n",
      "2025-05-17 13:37:27,717 - __main__ - INFO - üì§ Uploading FACT_Livraison to BigQuery ‚Üí pipeline-458019.vente.FACT_Livraison...\n",
      "2025-05-17 13:37:31,166 - __main__ - INFO - ‚úÖ Table FACT_Livraison uploaded successfully.\n",
      "2025-05-17 13:37:31,168 - __main__ - INFO - üì§ Uploading FACT_commandes to BigQuery ‚Üí pipeline-458019.vente.FACT_commandes...\n",
      "2025-05-17 13:37:34,451 - __main__ - INFO - ‚úÖ Table FACT_commandes uploaded successfully.\n",
      "2025-05-17 13:37:34,453 - __main__ - INFO - üì§ Uploading dim_produit to BigQuery ‚Üí pipeline-458019.vente.dim_produit...\n",
      "2025-05-17 13:37:37,364 - __main__ - INFO - ‚úÖ Table dim_produit uploaded successfully.\n",
      "2025-05-17 13:37:37,365 - __main__ - INFO - üì§ Uploading dim_entrepot to BigQuery ‚Üí pipeline-458019.vente.dim_entrepot...\n",
      "2025-05-17 13:37:41,489 - __main__ - INFO - ‚úÖ Table dim_entrepot uploaded successfully.\n",
      "2025-05-17 13:37:41,491 - __main__ - INFO - ‚úÖ Pipeline ex√©cut√© avec succ√®s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from datetime import date\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_csv_files(base_path):\n",
    "    \"\"\"Load CSV files from specified directory\"\"\"\n",
    "    df_product = pd.read_csv(f\"{base_path}\\\\produit.csv\")\n",
    "    df_entrepots = pd.read_csv(f\"{base_path}\\\\entrepots.csv\")\n",
    "    df_commandes = pd.read_csv(f\"{base_path}\\\\commandes.csv\")\n",
    "    df_livrasion = pd.read_csv(f\"{base_path}\\\\livrasion.csv\", sep=';')\n",
    "    df_mouvements = pd.read_csv(f\"{base_path}\\\\mouvements.csv\", sep=';')  \n",
    "    return df_product, df_entrepots, df_commandes, df_livrasion, df_mouvements\n",
    "\n",
    "def transformation_df(df):\n",
    "    \"\"\"Clean and transform dataframe\"\"\"\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    df['date_chargement'] = pd.Timestamp.now().date() \n",
    "    return df\n",
    "\n",
    "def create_sql_engine():\n",
    "    \"\"\"Create SQL Alchemy engine using environment variables\"\"\"\n",
    "    # Charger les variables d'environnement si ce n'est pas d√©j√† fait\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Utiliser la m√™me approche que celle qui fonctionne avec pyodbc\n",
    "    server_name = os.getenv('SQL_SERVER').replace('\\\\\\\\', '\\\\')\n",
    "    \n",
    "    # Construction de la cha√Æne de connexion ODBC exactement comme dans votre exemple fonctionnel\n",
    "    odbc_conn_str = (\n",
    "        f\"DRIVER={{{os.getenv('SQL_DRIVER')}}};\"\n",
    "        f\"SERVER={server_name};\"\n",
    "        f\"DATABASE={os.getenv('SQL_DATABASE')};\"\n",
    "        f\"UID={os.getenv('SQL_USERNAME')};\"\n",
    "        f\"PWD={os.getenv('SQL_PASSWORD')};\"\n",
    "        f\"TrustServerCertificate=yes;\"\n",
    "    )\n",
    "    \n",
    "    # Utilisation de la notation pyodbc:// avec la cha√Æne encod√©e\n",
    "    conn_str = f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(odbc_conn_str)}\"\n",
    "    \n",
    "    # Cr√©ation du moteur SQLAlchemy avec des param√®tres optimis√©s\n",
    "    return create_engine(\n",
    "        conn_str, \n",
    "        fast_executemany=True,  # Optimisation pour les insertions multiples\n",
    "        pool_pre_ping=True      # V√©rifie que la connexion est toujours active\n",
    "    )\n",
    "\n",
    "def load_to_sql(df, engine, name):\n",
    "    \"\"\"Load dataframe to SQL Server\"\"\"\n",
    "    logger.info(f\" ---Chargement de {name} dans SQL Server---\")\n",
    "    df.to_sql(\n",
    "        name=name,\n",
    "        con=engine,\n",
    "        if_exists='replace',\n",
    "        index=False,\n",
    "        method='multi',\n",
    "        chunksize=1000\n",
    "    )\n",
    "\n",
    "def generate_fact_tables(engine):\n",
    "    \"\"\"Generate fact tables from SQL queries\"\"\"\n",
    "    query_fact_commandes = \"\"\"\n",
    "    SELECT id_commande, prod.id_produit, dim.id_entrepot,\n",
    "    UPPER(DATENAME(MONTH, stg_com.date_commande)) AS Month_Name,\n",
    "    UPPER(DATENAME(YEAR, stg_com.date_commande)) AS Year_name,\n",
    "    COUNT(*) OVER() AS nombre_total_commandes,\n",
    "    SUM(prix_total) OVER() CA,\n",
    "    SUM(quantite) OVER(PARTITION BY stg_com.id_produit) quantite_vendu_par_produit\n",
    "    FROM [Fact_Commandes].[dbo].[stg_commandes] stg_com\n",
    "    LEFT JOIN [dbo].[dim_entrepot] dim ON dim.id_entrepot = stg_com.id_entrepot\n",
    "    LEFT JOIN [dbo].[dim_produit] prod ON prod.id_produit = stg_com.id_produit\n",
    "    \"\"\"\n",
    "    \n",
    "    query_fact_livraison = \"\"\"\n",
    "    SELECT dim.id_entrepot, prod.id_produit, id_client,\n",
    "    SUM(CASE WHEN statut_livraison = 'livr√©' THEN 1 ELSE 0 END) OVER() nbre_commandes_livr√©,\n",
    "    SUM(CASE WHEN statut_livraison = 'en retard' THEN 1 ELSE 0 END) OVER() nbre_commandes_en_retard,\n",
    "    SUM(CASE WHEN statut_livraison = 'en transit' THEN 1 ELSE 0 END) OVER() nbre_commandes_en_transit,\n",
    "    SUM(quantite) OVER() quantite_livr√©,\n",
    "    CASE WHEN quantite < 0 THEN 'Anomalies_livraisons' ELSE 'Pas anomalie' END AS Anomalies_livraisons,\n",
    "    DATEDIFF(day, date_commande, date_livraison) d√©lai_livraison\n",
    "    FROM [Fact_Commandes].[dbo].[stg_livrasion] stg_liv\n",
    "    LEFT JOIN [dbo].[dim_entrepot] dim ON dim.id_entrepot = CAST(stg_liv.entrepot_source AS varchar)\n",
    "    LEFT JOIN [dbo].[dim_produit] prod ON prod.id_produit = CAST(stg_liv.id_produit AS varchar)\n",
    "    \"\"\"\n",
    "    \n",
    "    query_fact_mouvement = \"\"\"\n",
    "    SELECT date_mouvement, PROD.id_produit, entr.id_entrepot,\n",
    "    SUM(CASE WHEN type_mouvement = 'r√©ception' THEN quantite ELSE 0 END) OVER(PARTITION BY stg_m.id_produit) Total_r√©ceptionn√©_par_produit,\n",
    "    SUM(CASE WHEN type_mouvement = 'exp√©dition' THEN quantite ELSE 0 END) OVER(PARTITION BY stg_m.id_produit) Total_exp√©diti√©_par_produit,\n",
    "    (SUM(CASE WHEN type_mouvement = 'r√©ception' THEN quantite ELSE 0 END) OVER(PARTITION BY stg_m.id_produit) - \n",
    "     SUM(CASE WHEN type_mouvement = 'exp√©dition' THEN quantite ELSE 0 END) OVER(PARTITION BY stg_m.id_produit)\n",
    "    ) AS Stock_th√©orique_par_produit,\n",
    "    (SUM(CASE WHEN type_mouvement = 'r√©ception' THEN quantite ELSE 0 END) OVER(PARTITION BY stg_m.id_entrepot) - \n",
    "     SUM(CASE WHEN type_mouvement = 'exp√©dition' THEN quantite ELSE 0 END) OVER(PARTITION BY stg_m.id_entrepot)\n",
    "    ) AS Stock_th√©orique_par_entrepot\n",
    "    FROM [Fact_Commandes].[dbo].[stg_mouvements] stg_m\n",
    "    LEFT JOIN [dbo].[dim_produit] PROD ON stg_m.id_produit = PROD.id_produit\n",
    "    LEFT JOIN [dbo].[dim_entrepot] entr ON stg_m.id_entrepot = entr.id_entrepot\n",
    "    \"\"\"\n",
    "\n",
    "    df_mouvement_fact = pd.read_sql(query_fact_mouvement, engine)\n",
    "    df_livraison_fact = pd.read_sql(query_fact_livraison, engine)\n",
    "    df_commandes_fact = pd.read_sql(query_fact_commandes, engine)\n",
    "    \n",
    "    return df_mouvement_fact, df_livraison_fact, df_commandes_fact\n",
    "\n",
    "def load_to_bigquery(facts, dataset_id, project_id, job_config):\n",
    "    \"\"\"Load data to BigQuery\"\"\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        os.getenv('GOOGLE_APPLICATION_CREDENTIALS'),\n",
    "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "    )\n",
    "    \n",
    "    client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "    \n",
    "    for table_name, df in facts.items():\n",
    "        table_id = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "        logger.info(f\"üì§ Uploading {table_name} to BigQuery ‚Üí {table_id}...\")\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "        logger.info(f\"‚úÖ Table {table_name} uploaded successfully.\")\n",
    "\n",
    "def main():\n",
    "    logger.info(\"üèÅ D√©but du pipeline\")\n",
    "    \n",
    "    # Load data\n",
    "    base_path = os.getenv('DATA_BASE_PATH')\n",
    "    df_product, df_entrepots, df_commandes, df_livrasion, df_mouvements = load_csv_files(base_path)\n",
    "    \n",
    "    # Transform data\n",
    "    df_product = transformation_df(df_product)\n",
    "    df_entrepot = transformation_df(df_entrepots)\n",
    "    df_commandes = transformation_df(df_commandes)\n",
    "    df_livrasion = transformation_df(df_livrasion)\n",
    "    df_mouvements = transformation_df(df_mouvements)\n",
    "    \n",
    "    # SQL operations\n",
    "    engine = create_sql_engine()\n",
    "    df_mouvement_fact, df_livraison_fact, df_commandes_fact = generate_fact_tables(engine)\n",
    "    \n",
    "    # Load to SQL\n",
    "    load_to_sql(df_product, engine, \"dim_produit\")\n",
    "    load_to_sql(df_entrepot, engine, \"dim_entrepot\")\n",
    "    load_to_sql(df_commandes, engine, \"stg_commandes\")\n",
    "    load_to_sql(df_livrasion, engine, \"stg_livrasion\")\n",
    "    load_to_sql(df_mouvements, engine, \"stg_mouvements\")\n",
    "    load_to_sql(df_mouvement_fact, engine, \"FACT_Mouvement\")\n",
    "    load_to_sql(df_livraison_fact, engine, \"FACT_Livraison\")\n",
    "    load_to_sql(df_commandes_fact, engine, \"FACT_commandes\")\n",
    "    \n",
    "    # Load to BigQuery\n",
    "    facts = {\n",
    "        \"FACT_Mouvement\": df_mouvement_fact,\n",
    "        \"FACT_Livraison\": df_livraison_fact,\n",
    "        \"FACT_commandes\": df_commandes_fact,\n",
    "        \"dim_produit\": df_product,\n",
    "        \"dim_entrepot\": df_entrepot\n",
    "    }\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "    load_to_bigquery(\n",
    "        facts=facts,\n",
    "        dataset_id=os.getenv('BQ_DATASET_ID'),\n",
    "        project_id=os.getenv('GCP_PROJECT_ID'),\n",
    "        job_config=job_config\n",
    "    )\n",
    "    \n",
    "    logger.info(\"‚úÖ Pipeline ex√©cut√© avec succ√®s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777e532-56b4-4241-91d2-ef8d319bf768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6351ec73-3c60-4342-a77a-e36440f4a57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 13:56:38,710 - __main__ - INFO - üèÅ D√©but du pipeline\n",
      "2025-05-17 13:56:39,325 - __main__ - INFO -  ---Chargement de dim_produit dans SQL Server---\n",
      "2025-05-17 13:56:44,475 - __main__ - INFO -  ---Chargement de dim_entrepot dans SQL Server---\n",
      "2025-05-17 13:56:50,568 - __main__ - INFO -  ---Chargement de stg_commandes dans SQL Server---\n",
      "2025-05-17 13:56:51,035 - __main__ - INFO -  ---Chargement de stg_livrasion dans SQL Server---\n",
      "2025-05-17 13:56:51,334 - __main__ - INFO -  ---Chargement de stg_mouvements dans SQL Server---\n",
      "2025-05-17 13:56:51,548 - __main__ - INFO -  ---Chargement de FACT_Mouvement dans SQL Server---\n",
      "2025-05-17 13:56:51,774 - __main__ - INFO -  ---Chargement de FACT_Livraison dans SQL Server---\n",
      "2025-05-17 13:56:51,942 - __main__ - INFO -  ---Chargement de FACT_commandes dans SQL Server---\n",
      "2025-05-17 13:56:52,334 - __main__ - INFO - üì§ Uploading FACT_Mouvement to BigQuery ‚Üí pipeline-458019.vente.FACT_Mouvement...\n",
      "2025-05-17 13:56:56,570 - __main__ - INFO - ‚úÖ Table FACT_Mouvement uploaded successfully.\n",
      "2025-05-17 13:56:56,572 - __main__ - INFO - üì§ Uploading FACT_Livraison to BigQuery ‚Üí pipeline-458019.vente.FACT_Livraison...\n",
      "2025-05-17 13:57:00,315 - __main__ - INFO - ‚úÖ Table FACT_Livraison uploaded successfully.\n",
      "2025-05-17 13:57:00,316 - __main__ - INFO - üì§ Uploading FACT_commandes to BigQuery ‚Üí pipeline-458019.vente.FACT_commandes...\n",
      "2025-05-17 13:57:04,299 - __main__ - INFO - ‚úÖ Table FACT_commandes uploaded successfully.\n",
      "2025-05-17 13:57:04,300 - __main__ - INFO - üì§ Uploading dim_produit to BigQuery ‚Üí pipeline-458019.vente.dim_produit...\n",
      "2025-05-17 13:57:13,779 - __main__ - INFO - ‚úÖ Table dim_produit uploaded successfully.\n",
      "2025-05-17 13:57:13,780 - __main__ - INFO - üì§ Uploading dim_entrepot to BigQuery ‚Üí pipeline-458019.vente.dim_entrepot...\n",
      "2025-05-17 13:57:16,635 - __main__ - INFO - ‚úÖ Table dim_entrepot uploaded successfully.\n",
      "2025-05-17 13:57:16,637 - __main__ - INFO - ‚úÖ Pipeline ex√©cut√© avec succ√®s\n",
      "2025-05-17 13:57:16,746 - __main__ - INFO - ‚úÖ Table dim_produit contient des donn√©es \n",
      "2025-05-17 13:57:16,759 - __main__ - INFO - ‚úÖ Table dim_entrepot contient des donn√©es \n",
      "2025-05-17 13:57:16,794 - __main__ - INFO - ‚úÖ Table stg_commandes contient des donn√©es \n",
      "2025-05-17 13:57:16,805 - __main__ - INFO - ‚úÖ Table stg_livrasion contient des donn√©es \n",
      "2025-05-17 13:57:16,816 - __main__ - INFO - ‚úÖ Table stg_mouvements contient des donn√©es \n",
      "2025-05-17 13:57:16,858 - __main__ - INFO - ‚úÖ Table FACT_Mouvement contient des donn√©es \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultat transform√© :\n",
      "   col1  col2 date_chargement\n",
      "0   1.0   1.0      2025-05-17\n",
      "4   1.0   2.0      2025-05-17\n",
      "‚úÖ Test FINAL r√©ussi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 13:57:16,919 - __main__ - INFO - ‚úÖ Table FACT_Livraison contient des donn√©es \n",
      "2025-05-17 13:57:16,987 - __main__ - INFO - ‚úÖ Table FACT_commandes contient des donn√©es \n",
      "2025-05-17 13:57:20,503 - __main__ - INFO - ‚úÖ Table BigQuery vente.FACT_Mouvement contient des donn√©es\n",
      "2025-05-17 13:57:23,350 - __main__ - INFO - ‚úÖ Table BigQuery vente.FACT_Livraison contient des donn√©es\n",
      "2025-05-17 13:57:26,180 - __main__ - INFO - ‚úÖ Table BigQuery vente.FACT_commandes contient des donn√©es\n",
      "2025-05-17 13:57:28,717 - __main__ - INFO - ‚úÖ Table BigQuery vente.dim_produit contient des donn√©es\n",
      "2025-05-17 13:57:31,276 - __main__ - INFO - ‚úÖ Table BigQuery vente.dim_entrepot contient des donn√©es\n",
      "2025-05-17 13:57:31,278 - __main__ - INFO - Test BigQuery r√©ussi - Toutes les tables contiennent des donn√©es\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import date\n",
    "\n",
    "%run ./pipeline_GCP.ipynb\n",
    "\n",
    "def test_transformation_df():\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"col1\": [1, None, None, 1, 1],\n",
    "        \"col2\": [1, None, None, 1, 2]\n",
    "    })\n",
    "    \n",
    "    transformed = transformation_df(df)\n",
    "    print(\"R√©sultat transform√© :\")\n",
    "    print(transformed)\n",
    "    \n",
    "\n",
    "    assert \"date_chargement\" in transformed.columns\n",
    "    assert transformed[\"date_chargement\"].iloc[0] == date.today()\n",
    "    assert len(transformed) == 2  \n",
    "    assert pd.isna(transformed[['col1', 'col2']]).sum().sum() == 0  \n",
    "    print(\"‚úÖ Test FINAL r√©ussi\")\n",
    "\n",
    "def create_sql_engine():\n",
    "    \"\"\"Create SQL Alchemy engine using environment variables\"\"\"\n",
    "    # Charger les variables d'environnement si ce n'est pas d√©j√† fait\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Utiliser la m√™me approche que celle qui fonctionne avec pyodbc\n",
    "    server_name = os.getenv('SQL_SERVER').replace('\\\\\\\\', '\\\\')\n",
    "    \n",
    "    # Construction de la cha√Æne de connexion ODBC exactement comme dans votre exemple fonctionnel\n",
    "    odbc_conn_str = (\n",
    "        f\"DRIVER={{{os.getenv('SQL_DRIVER')}}};\"\n",
    "        f\"SERVER={server_name};\"\n",
    "        f\"DATABASE={os.getenv('SQL_DATABASE')};\"\n",
    "        f\"UID={os.getenv('SQL_USERNAME')};\"\n",
    "        f\"PWD={os.getenv('SQL_PASSWORD')};\"\n",
    "        f\"TrustServerCertificate=yes;\"\n",
    "    )\n",
    "    \n",
    "    # Utilisation de la notation pyodbc:// avec la cha√Æne encod√©e\n",
    "    conn_str = f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(odbc_conn_str)}\"\n",
    "    \n",
    "    # Cr√©ation du moteur SQLAlchemy avec des param√®tres optimis√©s\n",
    "    return create_engine(\n",
    "        conn_str, \n",
    "        fast_executemany=True,  # Optimisation pour les insertions multiples\n",
    "        pool_pre_ping=True      # V√©rifie que la connexion est toujours active\n",
    "    )\n",
    "\n",
    "\n",
    "def test_sql_server_integration():\n",
    "    \"\"\"Teste que les donn√©es sont bien stock√©es dans SQL Server\"\"\"\n",
    "\n",
    "    engine = create_sql_engine()\n",
    "\n",
    "    tables_to_check = [\n",
    "        \"dim_produit\",\n",
    "        \"dim_entrepot\",\n",
    "        \"stg_commandes\",\n",
    "        \"stg_livrasion\",\n",
    "        \"stg_mouvements\",\n",
    "        \"FACT_Mouvement\",\n",
    "        \"FACT_Livraison\",\n",
    "        \"FACT_commandes\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    for table in tables_to_check:\n",
    "      try:  \n",
    "        df = pd.read_sql(f\"SELECT TOP 1 * FROM {table}\", engine)\n",
    "        if  df.empty :\n",
    "            logger.info(f\"la  Table {table} est vide\")\n",
    "        else :\n",
    "            logger.info(f\"‚úÖ Table {table} contient des donn√©es \")\n",
    "      except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur sur la table {table} : {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def test_bigquery_integration():\n",
    "    \"\"\"Teste que les donn√©es sont bien stock√©es dans BigQuery\"\"\"\n",
    "\n",
    "    credential_path = r\"C:\\Users\\user\\Pipeline_09_05_2025\\credential_path\\bigquery_credentials.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "    credential_path,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],)\n",
    "    client = bigquery.Client(credentials=credentials, project=\"pipeline-458019\")\n",
    "    \n",
    "    \n",
    "\n",
    "    tables_to_check = {\n",
    "        \"FACT_Mouvement\": \"vente\",\n",
    "        \"FACT_Livraison\": \"vente\",\n",
    "        \"FACT_commandes\": \"vente\" ,\n",
    "         \"dim_produit\" :\"vente\" ,\n",
    "         \"dim_entrepot\" :\"vente\"\n",
    "    }\n",
    "    \n",
    "    # 3. V√©rification de chaque table\n",
    "    for table, dataset in tables_to_check.items():\n",
    "        try:\n",
    "            query = f\"SELECT * FROM `pipeline-458019.{dataset}.{table}` LIMIT 1\"\n",
    "            df = client.query(query).to_dataframe()\n",
    "            assert not df.empty, f\"La table {table} est vide\"\n",
    "            logger.info(f\"‚úÖ Table BigQuery {dataset}.{table} contient des donn√©es\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur sur la table {dataset}.{table} : {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    logger.info(\"Test BigQuery r√©ussi - Toutes les tables contiennent des donn√©es\")\n",
    "\n",
    "def main():\n",
    "    test_transformation_df()\n",
    "    test_sql_server_integration()\n",
    "    test_bigquery_integration()\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c227ca-00fd-48b1-a811-1491613bebbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
